{"lang":"en","lang_factory":"trf","name":"trf_bertbaseuncased_lg","description":"Provides weights and configuration for the pretrained transformer model `bert-base-uncased`, published by Google Research. The package uses HuggingFace's `transformers` implementation of the model. Pretrained transformer models assign detailed contextual word representations, using knowledge drawn from a large corpus of unlabelled text. You can use the contextual word representations as features in a variety of pipeline components that can be trained on your own data.","notes":"Requires the `spacy-transformers` package to be installed. A CUDA-compatible GPU is advised for reasonable performance.","author":"Google Research (repackaged by Explosion)","email":"contact@explosion.ai","url":"https://github.com/explosion/spacy-transformers","license":"MIT","sources":[{"name":"bert-base-uncased","author":"Google Research","url":"https://github.com/google-research/bert"}],"version":"2.2.0","requirements":["spacy-transformers>=0.5.0"],"spacy_version":">=2.2.1","vectors":{"width":0,"vectors":0,"keys":0,"name":"spacy_pretrained_vectors"},"pipeline":["sentencizer","trf_wordpiecer","trf_tok2vec","trf_textcat"],"labels":{"trf_textcat":["POSITIVE","NEGATIVE"]},"factories":{"sentencizer":"sentencizer","trf_wordpiecer":"trf_wordpiecer","trf_tok2vec":"trf_tok2vec","trf_textcat":"textcat"}}